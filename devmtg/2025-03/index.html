<!--#include virtual="../../header.incl" -->

<div class="www_sectiontitle">Ninth LLVM Performance Workshop at CGO</div>

<ul>
  <li><b>What:</b> Ninth LLVM Performance Workshop at CGO</li>
  <li><b>When:</b> Sat 1 - Wed 5 March 2025</li>
  <li><b>Where:</b> Las Vegas, Nevada, United States</li>
  <a href="https://2025.cgo.org/attending/Venue" target="_blank">The Westin Las Vegas Hotel & Spa, 160 East Flamingo Road, Las Vegas, Nevada, USA, 89109</a> [In person]
  <li><b>Proposals should be submitted to:</b> <a href="https://easychair.org/conferences/?conf=llvmcgo2025
        ">Easychair Submission </a></li>
  <li><b>The deadline for receiving submissions is:</b> : January 25th, 2025</li>
  <li><b>Speakers will be notified of acceptance or rejection by:</b> February 1st, 2025</li>
  <li>Note: Travel grants are available to eligible candidates upon request. Please reach out to the program committee if you need travel grant for the workshop.</li>
  <li>Note: Invitation letters for visa application are available upon request. Please reach out to the program committee if you need invitation letter for visa application.</li>
</ul>

<p>
  The Ninth LLVM Performance Workshop will be held at
  (<a href="https://2025.cgo.org/track/cgo-2025-workshops-and-tutorials">CGO 2025</a>). The
  workshop is co-located with CC, HPCA, and PPoPP. If you are interested
  in attending the workshop, please register at the
  (<a href="https://conf.researchr.org/home/cgo-2025">CGO website</a>). The LLVM workshop
  at CGO will be in-person.
</p>

<p>
  Program Committee:
<ul>
  <li>Aditya (hiraditya at msn.com)</li>
  <li>Jose M Monsalve Diaz (jmonsalvediaz at anl.gov)</li>
  <li>Shilei Tian (i at tianshilei.me)</li>
  <li>Rafael Andres Herrera Guaitero (rafaelhg at udel.edu)</li>
  <li>Kevin Sala (kevin.sala at bsc.es)</li>

</ul>
</p>

<div class="www_sectiontitle" id="workshop-schedule">Schedule [WIP] </div>

<table width="100%" border="1">
  <tbody>
    <tr style="font-weight: bold">
      <td style="min-width: 160px;">
        <p>Time (EDT)</p>
      </td>
      <td>
        <p>Speaker</p>
      </td>
      <td>
        <p>Title</p>
      </td>
      <td>
        <p>Topic</p>
      </td>
    </tr>
    <!-- Opening remarks -->
    <tr>
      <td>
        <p>8:30 - 8:35 (5 min)</p>
      </td>
      <td>
        <p>
          <a href="https://www.linkedin.com/in/josemonsalve2/">Jose M Monsalve Diaz</a><br />
          <a href="https://www.linkedin.com/in/shiltian/">Shilei Tian</a><br />
          <a href="https://twitter.com/_hiraditya_">Aditya</a><br />
          <a href="https://www.linkedin.com/in/randres-herrera/">Rafael A Herrera Guaitero</a><br />
          <a href="https://www.linkedin.com/in/kevinsalapenades/">Kevin Sala</a>
        </p>
      </td>
      <td>
        <p>Opening Remarks</p>
      </td>
      <td>
        <p>Welcome and Introduction</p>
      </td>
    </tr>

    <!-- Lunch Break -->
    <tr>
      <td>
        <p>12:50 - 13:40 (50 min)</p>
      </td>
      <td>
        <p> - </p>
      </td>
      <td>
        <p> Lunch break </p>
      </td>
      <td>
        <p> - </p>
      </td>
    </tr>
    
    <!-- Open Discussion -->
    <tr>
      <td>
        <p>15:00 - 15:30 (30 min)</p>
      </td>
      <td>
        <p> - </p>
      </td>
      <td>
        <p> Open Discussion </p>
      </td>
      <td>
        <p> - </p>
      </td>
    </tr>
    <!-- Closing remarks -->
    <tr>
      <td>
        <p>15:30 - 15:40 (10 min)</p>
      </td>
      <td>
        <p>
          <a href="https://www.linkedin.com/in/josemonsalve2/">Jose M Monsalve Diaz</a><br />
          <a href="https://www.linkedin.com/in/shiltian/">Shilei Tian</a><br />
          <a href="https://twitter.com/_hiraditya_">Aditya</a><br />
          <a href="https://www.linkedin.com/in/randres-herrera/">Rafael A Herrera Guaitero</a><br />
          <a href="https://www.linkedin.com/in/kevinsalapenades/">Kevin Sala</a>
        </p>
      </td>
      <td>
        <p>Closing Remarks</p>
      </td>
      <td>
        <p>Getting feedback</p>
      </td>
    </tr>
  </tbody>
</table>


<div class="www_sectiontitle">Abstracts</div>
<!-- 
<h3 id="talk0" class="cgo-title" style="font-weight: bold;">Targeting NVIDIA Hopper using MLIR<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Guray Ozen<sup>1</sup></h4>
<h5><sup>1</sup>Google Research</h5>
<p>
  This talk explores how to make the most of the NVIDIA Hopper Tensor Core by using its new hardware features effectively. Even with these advanced features, the challenge lies in efficiently using them, especially when creating fast General Matrix Multiply (GEMM) kernels.

  The ongoing research focuses on integrating the features of the NVIDIA Hopper Architecture GPU into the MLIR compiler. The main goal is to turn MLIR into a strong compiler that can unlock the best performance from GPUs. The talk discusses how to implement important elements, such as the Tensor Memory Accelerator (TMA), warp-group level tensor core instructions, and transactional barriers.

  Several features have been implemented to GPU, NVGPU, and NVVM dialects. The GPU dialect is where we launch the kernel, NVVM dialect is where we generate PTX assembly, and NVGPU dialect is where we can create efficient kernels at a higher level. The talk covers a detailed discussion of these dialects and how they contribute to optimizing the performance of the NVIDIA Hopper Architecture.
</p>

<h3 id="talk1" class="cgo-title" style="font-weight: bold;">Map LLVM Values to corresponding source level expression<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Shivam Kunwar</h4>
<p>
  The primary objective of this project is to enhance the effectiveness of compiler-generated remarks and analysis reports for code optimization. These messages, while often comprehensive, lack direct connections to the corresponding source-level expressions. The goal is to bridge this gap by utilizing LLVM's intrinsic functions, which establish mappings between LLVM program entities and source-level expressions. The project specifically focuses on utilizing these intrinsic functions to generate or derive source expressions from LLVM values. This functionality is particularly important for enhancing memory access optimizations, including the reporting of memory access dependences that hinder vectorization.
</p>

<h3 id="talk2" class="cgo-title" style="font-weight: bold;">Using LLVM to inspect, and fix, the Physical Structure of a Large Scale Software - The Codevis Project<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Joachim Meyer<sup>1, 2</sup></h4>
<h5><sup>1</sup>KDE, <sup>2</sup>Codethink</h5>
<p>
  Codevis is an open source tool that enables the user to study, analyse, and fix large scale software architecture flaws.

  The software currently understands C, c++ and Fortran. Codevis' main use case is to display relationship graphs between libraries, structures (such as classes or pure "c" structures), functions (including traits, functions or methods), and files. Codevis offers several tools that help find problems on large scale designs, such as a `Knowledge Island` (a visualisation that showcases who originates the source code per file, module, class) to `Find Cycles`. We know old software usually grows organically, and without the knowledge we have today, cycles probably exist that make this an even more tangled ball of yarn.

  Codevis aims to help corporations visualise all their software architecture in one single tool. This ensures faster action can be taken during development, or faster action can be taken during architecture decision-making.

  LLVM is a core part of this project for the C++ and Fortran parser. It has been proven to work. It fixes architectural issues within the KDE Frameworks Libraries - a collection of 80+ libraries combinning millions of lines of code.
</p>

<h3 id="talk3" class="cgo-title" style="font-weight: bold;">Practical Use of BOLT<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Amir Ayupov<sup>1</sup></h4>
<h5><sup>1</sup>Meta</h5>
<p>
  BOLT is a binary optimizer for ELF binaries and is a part of LLVM project. Utilizing sample-based profiling, BOLT boosts the performance even for highly optimized binaries built with both profile-guided optimizations (PGO) and link-time optimizations (LTO).
  BOLT has been demonstrated to be effective for a number of workloads spanning from HHVM to Clang, Python, Rust, MySQL, and Chromium, and has features enabling its use in various environments. This talk focuses on practical aspects of BOLT application through profile collection, use of BOLT optimizations and flags for specific use cases, interaction with compiler PGO, and usage in continuous profiling scenarios.
</p>

<h3 id="talk4" class="cgo-title" style="font-weight: bold;">The Next 700 ML-Enabled Compiler Optimizations<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Venkatakeerthy S<sup>1</sup>, Siddharth Jain<sup>1</sup>, Umesh Kalvakuntla<sup>1</sup>, Pranav Sai Gorantla<sup>1</sup>, Rajiv Shailesh Chitale<sup>1</sup>, Eugene Brevdo<sup>2</sup>, Albert Cohen<sup>2</sup>, Mircea Trofin<sup>2</sup> and Ramakrishna Upadrasta<sup>1</sup></h4>
<h5><sup>1</sup>IIT Hyderabad, <sup>2</sup>Google DeepMind</h5>
<p>
  There is a growing interest in enhancing compiler optimizations with ML models, yet interactions between compilers and ML frameworks remain challenging. Some optimizations require tightly coupled models and compiler internals, raising issues with modularity, performance and framework independence. Practical deployment and transparency for the end-user are also important concerns. We propose MLCompiler-Bridge to enable ML model development within a traditional Python framework while making end-to-end integration with an optimizing compiler possible and efficient. We evaluate it on both research and production use cases, for training and inference, over several optimization problems, multiple compilers and its versions, and gym infrastructures.
</p>

<h3 id="talk5" class="cgo-title" style="font-weight: bold;">Unveiling the Power of Heterogeneous Computing: A Brief Dive into Host and Target Tasks with OpenMP LLVM<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Rafael Andres Herrera Guaitero<sup>1</sup>, Rodrigo Ceccato de Freitas<sup>2</sup>, Rémy Neveu<sup>3</sup> and Jose Manuel Monsalve Diaz<sup>3</sup></h4>
<h5><sup>1</sup>University of Delaware, <sup>2</sup>UNICAMP, <sup>3</sup>Argonne National Lab</h5>
<p>
  This session offers a concise overview of achieving heterogeneous computing through OpenMP. This presentation aims to explain the current state of the implementation and provide guidance to other developers, especially those who are new, on how to get started and contribute to LLVM's OpenMP host and target task support. We look at the role of the runtime system interface and its implementation, providing insights into the essential components that drive heterogeneous computing. We also discuss the current RFC of the offloading runtime project and the plans to use it universally. To help a practical understanding, we finish the session with a simple example that shows how to compile and execute heterogeneous code using the LLVM framework.
</p>

<h3 id="talk6" class="cgo-title" style="font-weight: bold;">Building a Fast Back-end for LLVM-IR<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Tobias Schwarz<sup>1</sup> and Alexis Engelke<sup>1</sup></h4>
<h5><sup>1</sup>Technical University of Munich</h5>
<p>
  Low compilation times of unoptimized builds are important for developer productivity and especially for fast start-up just-in-time compilation. LLVM's back-end is an often-cited problem for these use cases, with a substantial portion of the compile-time being spent for rewriting the program code multiple times.

  We develop a completely new LLVM back-end for a commonly used subset of LLVM-IR (e.g., typical output of Clang) targeting x86-64 without using the existing back-end infrastructure. Instead, we generate machine code with just two passes over the input IR without any further IR. This way, on the SPECint 2017 benchmarks, we achieve a ~10x compile-time speed-up over the LLVM -O0 back-end with a run-time slowdown in the range of 0-30%. Instead of building a custom IR, we start from LLVM-IR as this allows adopting our compiler as fast baseline without code changes while still providing an easy path to optimized compilation.

  In this talk, we describe our approach and related challenges and experiences when building an entirely new, performance-focused LLVM back-end from scratch.
</p>


<h3 id="talk7" class="cgo-title" style="font-weight: bold;">Dominance is not a Tree: Towards More Precise Dominance Relations<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>George Stelle<sup>1</sup>, Tarun Prabhu<sup>1</sup>, Pat McCormick<sup>1</sup></h4>
<h5><sup>1</sup>Los Alamos National Laboratory</h5>
<p>
  In LLVM and other modern compilers, single static assignment (SSA) is a crucial theory for internal representations, enabling optimizations in the presence of imperative code. A fundamental function of SSA is calculating and using dominance relations to determine where immutable variables can be referenced. Dominator trees have historically been a good approximation of the dominator relation and efficiently computable. However, there are programs for which the dominator tree fails to capture precise dominance relations, preventing optimizations. In this work, we give examples of these kinds of programs, and show how removing the restriction to tree relations enables more precise dominance relations, therefore enabling more optimization. We discuss how one can use properties of SSA to implement a more general dominance relation using a small set of trees corresponding to shared branches, which we call a dominator grove. We present a work-in-progress implementation of a dominator grove in LLVM, along with some of the current hurdles in modifying the existing analyses and transformations to be sound in the presence of non-tree dominance relations. Using the implementation, we collect empirical data on the frequency of non-tree dominance relations in real code. We present some basic formal properties of the approach, and end with a discussion of future work, including concurrent extensions to SSA theory.
</p>

<h3 id="keynote" class="cgo-title" style="font-weight: bold;">Arrays 2.0: Extending the Scope of the Array Abstraction<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Saman Amarasinghe<sup>1</sup></h4>
<h5><sup>1</sup> Massachusetts Institute of Technology</h5>
<p>
FORTRAN, the first programming language introduced over a half a century ago, ushered in multi-dimensional arrays to store data and FOR loops to iterate over them. Since then, the programming world has evolved to introduce a plethora of data structures ranging from lists and sets to trees and graphs. Yet, when it comes to handling immense data sets, arrays and loops remain a practical mainstay.  Every imperative programming language has implemented arrays and loops and most programmers learn to use them early on. Many important features in modern hardware from caches to prefetchers were created to efficiently execute array accesses by loops.  Even in LLVM, many optimizations including loop invariant code motion, vectorization and polyhedral optimizations were dedicated to optimizing loopy codes with array accesses. 
<br/>
The scope of the array data structure, a multi-dimensional, dense, integer grid of points in a rectilinear space, first introduced in FOTRAN, haven't expanded beyond this definition.  However, a significant portion of real-world data, either originating from sensors, computational processes, or human input, embodies innate structures such as sparsity, repeated value sequences, symmetry and continuous real-valued indices. These characteristics are evident in diverse fields like scientific computing, data analytics, graph processing, and machine learning.
<br/>
In this talk, I will attempt to expand the familiar arrays and loops abstraction beyond the original scope of FORTRAN with the TACO and Finch compilers. TACO pioneered auto-generating of kernels for any sparse tensor algebra operation across prevalent formats. Finch, on the other hand, has seamlessly integrated the management of structured data, capturing nuances like sparsity, repeated values, symmetry and arrays where indices are real values.
<br/>
I will demonstrate how to compile complex loopy programs with structured data into efficient loops in a systematic way and how our compiler's output rivals the performance of best-of-class handcrafted codes. I hope to convince you that we can finally put structured array programming on the same compiler transformation and code generation footing as dense array codes.

<br/>
<br/>
Saman Amarasinghe is a Professor in the Department of Electrical Engineering and Computer Science at Massachusetts Institute of Technology and a member of its Computer Science and Artificial Intelligence Laboratory (CSAIL) where he leads the Commit compiler group. Under Saman's guidance, the Commit group has developed a myriad of pioneering programming languages and compilers including the StreamIt, StreamJIT, PetaBricks, Halide, Simit, MILK, Cimple, TACO, GraphIt, BioStream, CoLa and Seq programming languages and compilers, DynamoRIO, Helium, Tiramisu, Codon and BuildIt compiler/runtime frameworks, Superword Level Parallelism (SLP), goSLP and VeGen for vectorization, Ithemal machine learning based performance predictor, Program Shepherding to protect programs against external attacks, the OpenTuner extendable autotuner, and the Kendo deterministic execution system. He was the co-leader of the Raw architecture project. Beyond academia, Saman was a co-founder of Determina, Lanka Internet Services Ltd., Venti Technologies, DataCebo and Exaloop corporations.  Saman received his BS in Electrical Engineering and Computer Science from Cornell University in 1988, and his MSEE and Ph.D. from Stanford University in 1990 and 1997, respectively.  He is an ACM Fellow.
</p>

<h3 id="talk8" class="cgo-title" style="font-weight: bold;">Automatic Parallelization and OpenMP Offloading of Fortran<br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span></h3>
<h4>Iva Ivanov<sup>1, 2</sup>, Jens Domke<sup>3</sup>, Toshio Endo<sup>1</sup>, Johannes Doerfert<sup>2</sup></h4>
<h5><sup>1</sup>Tokyo Institute of Technology, <sup>2</sup>Lawrence Livermore National Laboratory, <sup>3</sup>RIKEN Center for Computational Science (R-CCS)</h5>
<p>
  The most substantial compute power found in most modern HPC systems is in their accelerators, namely GPUs. Thus, it is extremely important to utilize them in order to maximize performance of scientific computing applications. Fortran is still prevalent in the scientific community and there are vast amounts of important existing applications written in it, however, legacy Fortran code was not written with accelerators in mind, so enabling scientists to easily make use of modern hardware with minimal effort is an important goal.

  OpenMP has been widely used as a way to accelerate these programs, and the 6.0 version of the standard which is scheduled to be released in late 2024 introduces a new directive with this goal in mind, called coexecute. It allows the programmer to instruct the compiler to automatically parallelize and offload a sequence of array operations and calls to intrinsic functions. This requires extensive compiler transformations such as splitting device kernels and parallelization of loops, which we implement in LLVM's MLIR based compiler, Flang. We show how automatic parallelization and offloading of existing fortran code to accelerators is possible with just simple annotations from the programmer.
</p> -->

<div class="www_sectiontitle">Call for Speakers</div>
<p>
  We invite speakers from academia and industry to present their work on the following list of topics (including and not
  limited to:)
</p>
<ul>
  <li>Improving performance and code-size of applications built by LLVM toolchains</li>
  <li>Improving performance of LLVM's runtime libraries</li>
  <li>Improving the security of generated code</li>
  <li>Any tools or products developed by using one of the libraries in LLVM infrastructure</li>
  <li>Performance tracking over time</li>
  <li>Compiler flags, annotations and remarks to understand and improve performance</li>
  <li>Any other topic related to improving and maintaining the performance and quality of LLVM generated code</li>
</ul>
<p>

  While the primary focus of the workshop is on these topics, we welcome
  any submission related to the LLVM-project, its sub-projects (clang,
  mlir, lldb, Polly, lld, openmp, pstl, compiler-rt, etc.), as well as
  their use in industry and academia.
</p>

<p>We are looking for:</p>
<ul>
  <li>keynote speakers (30-60minutes),</li>
  <li>technical presentations (30 minutes plus questions and discussion),</li>
  <li>tutorials (30-60minutes),</li>
  <li>panels (30-60minutes),</li>
  <li>BOFs (30-60minutes)</li>
</ul>

<p>
  Proposals should provide sufficient information for the review
  committee to be able to judge the quality of the submission. Proposals
  can be submitted under the form of an extended abstract, full paper,
  or slides. Accepted presentations will be presented
  online. The presentations will be publicly available on
  <a href="https://llvm.org/devmtg/">https://llvm.org/devmtg/</a>
</p>

<p>
  In case of any queries please reach out to the workshop organizers:
  Aditya (hiraditya at msn.com), Jose M Monsalve Diaz (jmonsalvediaz at anl.gov),
  Shilei Tian (i at tianshilei.me), Rafael (rafaelhg at udel.edu),
  or Kevin Sala (kevin.sala at bsc.es).
</p>

<h4>What types of people attend?</h4>
<ul>
  <li>Active developers of projects in the LLVM Umbrella (LLVM core, Clang, LLDB, libc++, compiler_rt, klee, lld,
    OpenMP, etc).</li>
  <li>Anyone interested in using these as part of another project.</li>
  <li>Students and Researchers.</li>
  <li>Compiler, programming language, and runtime enthusiasts.</li>
  <li>Those interested in using compiler and toolchain technology in novel and interesting ways.</li>
</ul>


<h4>Panels</h4>
Panel sessions are guided discussions about a specific topic. The panel consists of ~3 developers who discuss a topic
through prepared questions from a moderator. The audience is also given the opportunity to ask questions of the panel.

<h4>Birds of a Feather (BoF)</h4>
A BoF session, an informal meeting at conferences, where the attendees group together based on a shared interest and
carry out discussions without any pre-planned agenda.

<h4>Technical Talks</h4>
These 20-30 minute talks cover all topics from core infrastructure talks, to project's using LLVM's infrastructure.
Attendees will take away technical information that could be pertinent to their project or general interest.

<h4>Tutorials</h4>
Tutorials are 30-60 minute sessions that dive down deep into a technical topic. Expect in depth examples and
explanations.

<div class="www_sectiontitle" id="coc">Code of Conduct</div>
<p>The LLVM Foundation is dedicated to providing an inclusive and safe
  experience for everyone. We do not tolerate harassment of participants in any
  form. By registering for this event, we expect you to have read and agree to
  the <a href="http://llvm.org/docs/CodeOfConduct.html">LLVM Code of Conduct</a>.

  We also adhere to the <a href="https://conf.researchr.org/attending/cgo-2025/code-of-conduct" target="_blank">
  CGO Code of Conduct</a>.
</p>


<!-- *********************************************************************** -->
<hr>

<!--#include virtual="../../footer.incl" -->
